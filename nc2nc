#!/usr/bin/env python
from netCDF4 import Dataset
import numpy as np
import numpy.ma as ma
import sys
import math
import operator
import itertools as it

dtypes = {
    'f' : 4, # f4, 32-bit floating point
    'd' : 8, # f8, 64-bit floating point
    'e' : 4, # f2, 16-bit floating point
    'i' : 4, # i4, 32-bit signed integer
    'h' : 2, # i2, 16-bit signed integer
    'l' : 8, # i8, 64-bit singed integer
    'b' : 1, # i1, 8-bit signed integer
    'B' : 1, # u1, 8-bit unsigned integer
    'H' : 2, # u2, 16-bit unsigned integer
    'I' : 4, # u4, 32-bit unsigned integer
    'L' : 8, # u8, 64-bit unsigned integer
    'S' : 1 }  # S1, single-character string

def numVals(shape):
    """Return number of values in chunk of specified shape, given by a list of dimension lengths.

    shape -- list of variable dimension sizes"""
    if(len(shape) == 0):
        return 1
    return reduce(operator.mul, shape)

def calcChunkShape(chunkVol, varShape):
    """
    Calculate a chunk shape for a given volume/area for the dimensions in varShape.

    chunkVol   -- volume/area of the chunk
    chunkVol   -- array of dimensions for the whole dataset
    """

    return np.array(np.ceil(np.asarray(varShape) * (chunkVol / float(numVals(varShape))) ** (1./len(varShape))),dtype="int")

def chunk_shape_nD(varShape, valSize=4, chunkSize=4096, minDim=2):
    """
    Return a 'good shape' for an nD variable, assuming balanced 1D, 2D access

    varShape  -- list of variable dimension sizes
    chunkSize -- minimum chunksize desired, in bytes (default 4096)
    valSize   -- size of each data value, in bytes (default 4)
    minDim    -- mimimum chunk dimension (if var dimension larger
                 than this value, otherwise it is just var dimension)

    Returns integer chunk lengths of a chunk shape that provides
    balanced access of 1D subsets and 2D subsets of a netCDF or HDF5
    variable var. 'Good shape' for chunks means that the number of
    chunks accessed to read any kind of 1D or 2D subset is approximately
    equal, and the size of each chunk (uncompressed) is at least
    chunkSize, which is often a disk block size.
    """

    varShapema = ma.array(varShape)
    
    chunkVals = min(chunkSize / float(valSize),numVals(varShapema)) # ideal number of values in a chunk

    # Make an ideal chunk shape array 
    chunkShape = ma.array(calcChunkShape(chunkVals,varShapema),dtype=int)

    # Short circuit for 1D arrays. Logic below unecessary & can have divide by zero
    if len(varShapema) == 1: return chunkShape.filled(fill_value=1)

    # And a copy where we'll store our final values
    chunkShapeFinal = ma.masked_all(chunkShape.shape,dtype=int)

    lastChunkCount = -1
    
    while True:

        # Loop over the axes in chunkShape, making sure they are at
        # least minDim in length.
        for i in range(len(chunkShape)):
            if ma.is_masked(chunkShape[i]):
                continue 
            if (chunkShape[i] < minDim):
                # Set the final chunk shape for this dimension
                chunkShapeFinal[i] = min(minDim,varShapema[i])
                # mask it out of the array of possible chunkShapes
                chunkShape[i] = ma.masked

        # print chunkShape,chunkShapeFinal
        # Have we fixed any dimensions and filled them in chunkShapeFinal?
        if chunkShapeFinal.count() > 0:
            chunkCount = numVals(chunkShapeFinal[~chunkShapeFinal.mask])
        else:
            if (lastChunkCount == -1):
                # Haven't modified initial guess, break out of
                # this loop and accept chunkShape 
                break

        if chunkCount != lastChunkCount:
            # Recalculate chunkShape array, with reduced dimensions
            chunkShape[~chunkShape.mask] = calcChunkShape(chunkVals/chunkCount,varShapema[~chunkShape.mask])
            lastChunkCount = chunkCount
        else:
            break

    # This doesn't work when chunkShape has no masked values. Weird.
    # chunkShapeFinal[chunkShapeFinal.mask] = chunkShape[~chunkShape.mask]
    for i in range(len(chunkShapeFinal)):
        if ma.is_masked(chunkShapeFinal[i]):
            chunkShapeFinal[i] = chunkShape[i]

    return chunkShapeFinal.filled(fill_value=1)


def nc2nc(filename_o, filename_d, zlib=True, complevel=4, shuffle=True, fletcher32=False,
    clobber=False, lsd_dict=None, verbose=False, classic=0, vars=None, copyBuffer=51200):
    """convert a netcdf file (filename_o) to a netcdf file
    The default format is 'NETCDF4', but can be set
    to NETCDF4_CLASSIC if classic=1.
    If the lsd_dict is not None, variable names
    corresponding to the keys of the dict will be truncated to the decimal place
    specified by the values of the dict.  This improves compression by
    making it 'lossy'..
    If vars is not None, only variable names in the list 
    will be copied (plus all the dimension variables).
    The zlib, complevel and shuffle keywords control
    how the compression is done."""

    ncfile_o = Dataset(filename_o,'r')
    if classic:
        ncfile_d = Dataset(filename_d,'w',clobber=clobber,format='NETCDF4_CLASSIC')
    else:
        ncfile_d = Dataset(filename_d,'w',clobber=clobber,format='NETCDF4')
    mval = 1.e30 # missing value if unpackshort=True

    # Copy buffer specified in KB, so convert to bytes
    copyBuffer = 1024*copyBuffer

    # create dimensions. Check for unlimited dim.
    unlimdimname = False
    unlimdim = None

    # create global attributes.
    if verbose: sys.stdout.write('copying global attributes ..\n')
    #for attname in ncfile_o.ncattrs():
    #    setattr(ncfile_d,attname,getattr(ncfile_o,attname))
    ncfile_d.setncatts(ncfile_o.__dict__) 

    # Copy dimensions
    if verbose: sys.stdout.write('copying dimensions ..\n')
    for dimname,dim in ncfile_o.dimensions.items():
        if dim.isunlimited():
            unlimdimname = dimname
            unlimdim = dim
            ncfile_d.createDimension(dimname,None)
        else:
            ncfile_d.createDimension(dimname,len(dim))

    # create variables.
    if vars is None:
       varnames = ncfile_o.variables.keys()
    else:
       # variables to copy specified
       varnames = vars
       # add dimension variables
       for dimname in ncfile_o.dimensions.keys():
           if dimname in ncfile_o.variables.keys() and dimname not in varnames:
               varnames.append(dimname)

    for varname in varnames:
        ncvar = ncfile_o.variables[varname]
        if verbose: sys.stdout.write('copying variable %s\n' % varname)
        # quantize data?
        if lsd_dict is not None and lsd_dict.has_key(varname):
            lsd = lsd_dict[varname]
            if verbose: sys.stdout.write('truncating to least_significant_digit = %d\n'%lsd)
        else:
            lsd = None # no quantization.
        datatype = ncvar.dtype

        # is there an unlimited dimension?
        if unlimdimname and unlimdimname in ncvar.dimensions:
            hasunlimdim = True
        else:
            hasunlimdim = False

        if hasattr(ncvar, '_FillValue'):
            FillValue = ncvar._FillValue
        else:
            FillValue = None 

        chunksizes = None
        # check we have a mapping from the type to a number of bytes
        if ncvar.dtype.char in dtypes: 
            if verbose: sys.stdout.write('Variable shape: %s\n' % str(ncvar.shape))
            chunksizes=chunk_shape_nD(ncvar.shape,valSize=dtypes[ncvar.dtype.char])
            if verbose: sys.stdout.write('Chunk sizes: %s\n' % str(chunksizes))
        else:
            sys.stderr.write("This datatype not supported: dtype : %s\n" % ncvar.dtype.char)
            sys.exit(1)

        # Create the variable we will copy to
        var = ncfile_d.createVariable(varname, datatype, ncvar.dimensions, fill_value=FillValue, least_significant_digit=lsd, zlib=zlib, complevel=complevel, shuffle=shuffle, fletcher32=fletcher32, chunksizes=chunksizes)
        # fill variable attributes.
        attdict = ncvar.__dict__
        if '_FillValue' in attdict: del attdict['_FillValue']
        var.setncatts(attdict)

        # fill variable with data.

        dimlim = np.asarray(ncvar.shape)

        # bufferChunk is a multiple of the chunksize which is less than the size of copy buffer
        bufferChunk = chunk_shape_nD(ncvar.shape,valSize=dtypes[ncvar.dtype.char],chunkSize=copyBuffer)

        # Don't bother copying in steps if all our data fits inside the bufferChunk
        if np.all(bufferChunk >= dimlim):
            var[:] = ncvar[:]
        else:

            # Make sure our chunk size is no larger than the dimension in that direction
            for ind, chunk in enumerate(bufferChunk):
                if chunk > dimlim[ind]: bufferChunk[ind] = dimlim[ind]
    
            if verbose: sys.stdout.write('Buffer chunk : %s\n' % str(bufferChunk))

            # bufferSteps is the number of copies of bufferChunk that fit along each axis
            bufferSteps = (dimlim-1)/bufferChunk + 1
    
            # Make an iterator out of all possible combinations of the bufferOffsets, which
            # are just steps along each dimension
            for index in np.ndindex(*bufferSteps):
                index *= bufferChunk
                slices = []
                for start, step, end in it.izip(index, bufferChunk, dimlim):
                    # There are two checks in the 2nd pat of the slice, the first
                    # (min) makes sure we don't go beyond the limits of the variable
                    # and the second makes sure the slice has at least one element.
                    # Chunksizes of 1 lead to zero otherwise. 
                    slices.append(slice(start,min(start+step,end),None))
                var[slices] = ncvar[slices] 

        ncfile_d.sync() # flush data to disk

    # close files.
    ncfile_o.close()
    ncfile_d.close()

if __name__ == '__main__':

    import getopt, os

    usage = """
 Copy a netCDF file to netCDF format, and adding zlib compression (with the HDF5 shuffle filter and fletcher32 checksum).

 usage: %s [-h] [-o] [--vars=var1,var2,..] [--zlib=(0|1)] [--complevel=(1-9)] [--shuffle=(0|1)] [--fletcher32=(0|1)] [--quantize=var1=n1,var2=n2,..] origin_file destination_file

 -h           - Print usage message.
 -o           - Overwite destination file (default is to raise an error if output file already exists).
 --vars       - comma separated list of variable names to copy (default is to copy all variables)
 --classic    - use NETCDF4_CLASSIC format instead of NETCDF4 (default 1)
 --zlib       - Activate (or disable) zlib compression (default is activate).
 --complevel  - Set zlib compression level (4 is default).
 --shuffle    - Activate (or disable) the shuffle filter (active by default).
 --fletcher32 - Activate (or disable) the fletcher32 checksum (not active by default).
 --quantize   - Truncate the data in the specified variables to a given decimal precision.
                For example, 'speed=2, height=-2, temp=0' will cause the variable 'speed' to be truncated
                to a precision of 0.01, 'height' to a precision of 100 and 'temp' to 1. This can significantly
                improve compression. The default is not to quantize any of the variables. (comma separated list
                of "variable name=integer" pairs)
 --copybuffer - size of the buffer to use for copying in kB (default: 51200 = 50MB)
 --verbose    - if 1 print diagnostic information.
\n""" % os.path.basename(sys.argv[0])

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'ho', ['classic=', 'vars=', 'zlib=', 'verbose', 'complevel=',
                                                         'shuffle=', 'fletcher32=', 'copybuffer=','quantize='])
    except:
        (type, value, traceback) = sys.exc_info()
        sys.stdout.write("Error parsing the options. The error was: %s\n" % value)
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    overwritefile = 0
    complevel = 6
    classic = 1
    zlib = 1
    shuffle = 1
    fletcher32 = 0
    vars = None
    quantize = None
    verbose = 0
    copybuffer = 51200

    # Get the options
    for option in opts:
        if option[0] == '-h':
            sys.stderr.write(usage)
            sys.exit(0)
        elif option[0] == '-o':
            overwritefile = 1
        elif option[0] == '--classic':
            classic = int(option[1])
        elif option[0] == '--zlib':
            zlib = int(option[1])
        elif option[0] == '--verbose':
            verbose = 1
        elif option[0] == '--complevel':
            complevel = int(option[1])
        elif option[0] == '--shuffle':
            shuffle = int(option[1])
        elif option[0] == '--fletcher32':
            fletcher32 = int(option[1])
        elif option[0] == '--vars':
            vars = option[1]
        elif option[0] == '--quantize':
            quantize = option[1]
        elif option[0] == '--copybuffer':
            copybuffer = int(option[1])
        else:
            sys.stdout.write("%s: Unrecognized option\n" % option[0])
            sys.stderr.write(usage)
            sys.exit(0)

    # if we pass a number of files different from 2, abort
    if len(pargs) < 2 or len(pargs) > 2:
        sys.stdout.write("You need to pass both source and destination!.\n")
        sys.stderr.write(usage)
        sys.exit(0)

    # Catch the files passed as the last arguments
    filename_o = pargs[0]
    filename_d = pargs[1]

    # Parse the quantize option, create a dictionary from key/value pairs.
    if quantize is not None:
        lsd_dict = {}
        for p in quantize.split(','):
            kv = p.split('=')
            lsd_dict[kv[0]] = int(kv[1])
    else:
        lsd_dict=None

    # Parse the vars option, create a list of variable names.
    if vars is not None:
       vars = vars.split(',')

    # copy the data from filename_o to filename_d.
    nc2nc(filename_o,filename_d, zlib=zlib, complevel=complevel, shuffle=shuffle,
        fletcher32=fletcher32, clobber=overwritefile, lsd_dict=lsd_dict,
        verbose=verbose,vars=vars, classic=classic, copyBuffer=copybuffer)
